[
  {
    "content": "Logs are retained based on size; once the log size exceeds 1GB, older segments are deleted.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Logs are retained for exactly one day, regardless of the size of the log.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Logs are retained until the size of the log exceeds 1GB or for one day, whichever comes first.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Logs are retained indefinitely, as `log.retention.bytes` is set to -1, overriding other retention configurations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Although `log.segment.bytes` is set to 1GB, this setting alone does not dictate log retention based on size. It specifies the maximum size of a single log segment file. The deletion policy based on size is controlled by `log.retention.bytes`, which is not effectively set here due to its value being -1 (indicating no limit).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. The setting `log.retention.ms` = 86400000 specifies that logs are retained for 86400000 milliseconds, which is equivalent to 24 hours or one day. This means logs are deleted after one day, regardless of their size, as long as `log.retention.bytes` does not impose a stricter limit, which in this case, it does not (`log.retention.bytes` = -1).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. This statement misinterprets the settings. Kafka does not use both `log.retention.ms` and `log.retention.bytes` to determine a \"whichever comes first\" policy. Instead, both conditions must be met for a log segment to be eligible for deletion. Given `log.retention.bytes` = -1, size-based deletion is effectively disabled, leaving time-based deletion as the operative policy.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The statement that logs are retained indefinitely is wrong because `log.retention.ms` is explicitly set to a finite duration (86400000 ms or 1 day), dictating a time-based retention policy. The `-1` value for `log.retention.bytes` means there is no size limit on log retention, but it does not affect or override the time-based retention set by `log.retention.ms`.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Log compaction and deletion are mutually exclusive; only one policy can be active at any time.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Log compaction will occur once 50% of the segment data is marked as dirty, and logs older than 1 day will be deleted.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Deleted records are removed immediately from the log; `delete.retention.ms` specifies the retention time for all records.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`segment.ms` dictates the maximum lifespan of any record in the log, after which it is eligible for compaction or deletion.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The configuration `cleanup.policy` = \"compact,delete\" allows for both log compaction and deletion policies to be applied to the same topic. This means that the log will undergo compaction to remove duplicates and retain only the latest value for each key, and log segments can also be deleted based on time or size constraints.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. With `min.cleanable.dirty.ratio` set to 0.5, log compaction is triggered when at least 50% of the segment is considered dirty, which means it contains records that are either duplicates or marked for deletion. The `delete.retention.ms` setting ensures that a record marked for deletion remains in the log for an additional day after being deleted, which allows for consumer recovery in case of deletion mistakes before the log is compacted.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Deleted records are not removed immediately; instead, they are marked for deletion and actually removed during the next compaction cycle. The `delete.retention.ms` parameter specifies the time a deleted record is retained before being permanently removed from the log as part of compaction, not the retention time for all records.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. `segment.ms` specifies the time after which Kafka will close the current log segment and create a new one. It does not dictate the maximum lifespan of any record in the log. Record lifespan is determined by the `cleanup.policy` and associated configurations like `log.retention.ms` for deletion and `min.cleanable.dirty.ratio` for compaction.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Elected by broker majority.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Elected by Zookeeper ensemble.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Responsible for partition leader election.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Manages consumer group offsets.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Automatically assigns replicas to brokers based on load.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "###",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The Controller in a Kafka cluster is not elected by a broker majority. The election process is not based on a majority vote among the brokers themselves.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. The Controller is elected by the Zookeeper ensemble. Kafka uses Zookeeper to manage cluster metadata and to perform leader election for the controller. When the current Controller fails or loses connection to Zookeeper, a new Controller is elected by Zookeeper from among the live members of the cluster.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. One of the primary responsibilities of the Controller is to manage partition leader elections. When a partition leader fails, the Controller is responsible for choosing a new leader from the set of in-sync replicas (ISRs) and updating the cluster metadata accordingly.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Managing consumer group offsets is not a responsibility of the Controller. Consumer group offsets are maintained by Kafka brokers, with the offsets being stored either in a dedicated __consumer_offsets topic within Kafka (for newer versions) or Zookeeper (for older versions).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Automatically assigning replicas to brokers based on load is not directly managed by the Controller. Replica assignment is typically done at topic creation time or during manual rebalancing operations. While the Controller does manage some aspects of replica management, such as initiating reassignment tasks, the automatic balancing of load is not a direct responsibility of the Controller but can be achieved through tools like Kafka's built-in partition reassignment tool or third-party solutions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If a broker acting as the Controller goes down, what mechanism is responsible for the election of a new Controller?",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "A partition leader fails, and all its replicas are on brokers with the same network latency to the Zookeeper ensemble. How is the new leader chosen among the replicas?",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "During a network partition, a subset of brokers becomes isolated from the main cluster. What determines which brokers will continue to serve as leaders for their partitions?",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**- A. The Zookeeper ensemble elects the new Controller based on ephemeral node creation sequence.**",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "When the broker acting as the Controller fails, Zookeeper plays a crucial role in the election of a new Controller. Kafka brokers register themselves with Zookeeper using ephemeral nodes. When the current Controller's node disappears from Zookeeper (due to failure or disconnection), Zookeeper triggers the Controller re-election process among the live brokers. The new Controller is typically the first broker to respond to this trigger, based on the sequence of ephemeral node creation.",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**B. The new partition leader is elected based on the ISR list order, favoring replicas with the most recent updates.**",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "Kafka does not use a random process or explicit network latency measurements to select a new leader among replicas. Instead, it relies on the ordered list of in-sync replicas (ISRs) for each partition. The new leader is usually the first replica in the ISR list that is still available. This mechanism ensures that the chosen leader is up-to-date with the latest messages to prevent data loss.",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**C. Brokers in the main cluster segment with access to Zookeeper retain their roles, while isolated brokers step down until connectivity is restored.**",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "In the event of a network partition that isolates a subset of brokers, the decision on which brokers continue to serve as leaders for their partitions depends on their ability to communicate with Zookeeper. Brokers on the side of the partition that maintains connectivity to Zookeeper continue to function normally, retaining their roles. Meanwhile, isolated brokers lose their leadership status for partitions and step down, becoming followers if they are part of the ISR and can establish leadership once connectivity is restored and they rejoin the cluster. This ensures the cluster remains operational and consistent, prioritizing segments with Zookeeper connectivity.",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The producer will be able to publish messages, but with potential data loss.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The producer will temporarily be unable to publish messages until at least one replica broker comes back online.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The producer will continue to publish messages successfully without any impact.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The producer will immediately switch to another topic's partition that has all replicas available.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. With `acks=all`, the producer requires acknowledgments from the leader and all in-sync replicas (ISRs) to consider a message write successful. If Brokers B and C are offline, it's not a matter of potential data loss but rather that the producer cannot achieve the required acknowledgments from all replicas, as they are not available to replicate the data.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. The `acks=all` setting ensures that the producer receives acknowledgments from the partition leader and all replicas before considering a message successfully published. If Brokers B and C, hosting the replicas, go offline, the condition for `acks=all` cannot be met because these replicas cannot acknowledge the message replication. As a result, the producer will be unable to publish new messages to this partition until at least one of the replicas (Broker B or C) becomes available again and can acknowledge the message replication alongside the leader (Broker A). If Brokers B and C were considered part of the ISR list before going down, the producer would be unable to publish messages because it expects acknowledgments from all ISRs, which now includes unavailable brokers. This state creates a temporary inability to publish new messages, as the producer cannot satisfy its acknowledgment requirements. However, the immediate effect of brokers going offline is that they are removed from the ISR for that partition. If the leader (Broker A) remains online but all replicas are offline, the ISR shrinks to include only the leader. In real-time operation, Kafka aims to maintain availability and durability, so the producer can still publish messages, but only if at least one replica comes back online to fulfill the acks=all requirement of replicating to all in-sync replicas. This detail was overlooked in the initial explanation. The immediate effect of the replica brokers going offline is that they are removed from the in-sync replica (ISR) list for the partition. Note that if the leader (Broker A) remains online, the producer can still publish messages to the partition, but the `acks=all` requirement ensures that the messages are replicated to all available ISRs before being considered successfully published. So, the producer's ability to publish depends on the presence of at least one ISR, which in this case would be just the leader until a replica comes back online.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The `acks=all` configuration means that the producer expects acknowledgments from all replicas to ensure data durability. If the replicas are down, the producer won't be able to receive acknowledgments from all required parties, impacting its ability to successfully publish messages. It's not about continuing without impact; the producer will face a temporary blockade in publishing messages.  Kafka aims to maintain a balance between availability and durability. While the `acks=all` setting prioritizes durability, Kafka's design allows for continued operation with a reduced ISR to maintain availability, as long as the minimum ISR count is met.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Kafka producers do not automatically switch to another topic's partition in response to issues with the current partition's replicas. The producer's target partition is determined by the partitioning logic (either default or custom) at the time of message production. Failures of replicas in a partition do not trigger automatic redirection of messages to different partitions within the same or different topics.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Registering itself with Zookeeper.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Loading the replica assignment for each partition it hosts.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Creating a new Zookeeper znode for each topic it has partitions for.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Initializing the log directories for each partition it hosts.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. When a Kafka broker starts up, one of the first tasks it performs is registering itself with Zookeeper. It creates an ephemeral znode under the `/brokers/ids` path in Zookeeper, which signifies its presence in the cluster.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The broker loads the replica assignment for each partition it hosts from Zookeeper during startup. This information is stored in Zookeeper under the `/brokers/topics` path and contains the mapping of partitions to their assigned replicas.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. A Kafka broker does not create new Zookeeper znodes for each topic it has partitions for during startup. The topic znodes are created when the topics themselves are created, either through the Kafka admin tools or via the Kafka broker when it receives a request to create a new topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Initializing the log directories for each partition it hosts is an essential task performed by the broker during startup. It ensures that the necessary directory structure and files are in place for storing and managing the partition data.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The Controller periodically triggers a rebalance operation to redistribute partition leadership.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The Controller assigns leadership to the broker with the least number of leader partitions for each new partition.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The Controller uses a round-robin algorithm to assign leadership across brokers.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The Controller does not actively manage the distribution of partition leadership among brokers.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The Controller does not periodically trigger rebalance operations to redistribute partition leadership. Rebalancing is typically initiated by Kafka administrators or automated tools based on cluster performance and resource utilization.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The Controller does not assign leadership based on the number of leader partitions each broker currently has. When a partition leader needs to be elected, the Controller selects the first replica in the ISR (in-sync replica) list, regardless of the broker's current leadership count.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The Controller does not use a round-robin algorithm to assign leadership across brokers. The leader election process is based on the ISR list and the availability of replicas, rather than a predetermined order or rotation.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. The Controller's primary responsibility is to manage partition states and elect partition leaders when necessary, such as when a broker fails or a new partition is created. However, it does not actively aim to evenly distribute partition leadership among brokers. The distribution of partition leadership is a result of factors like topic creation, replica assignment, and broker failures, rather than being actively managed by the Controller.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Partitions with a leader in Group A will continue to function normally, while partitions with a leader in Group B will be offline until the network partition is resolved.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Partitions with a leader in Group B will continue to function normally, while partitions with a leader in Group A will elect new leaders from the ISRs in Group B.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "All partitions will be offline until the network partition is resolved, as the brokers cannot reach a quorum for leader election.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The behavior is non-deterministic and depends on which group the Controller belongs to.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. In a network partition scenario where both groups can communicate with Zookeeper, Kafka prioritizes availability and allows the larger group (Group B in this case) to continue operating normally. Partitions with a leader in Group A will not continue to function normally, as they do not have a majority of ISRs available.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. Kafka's partition leadership election process is designed to ensure that the cluster remains available and functional in the presence of network partitions. When a network partition occurs, Kafka follows the \"majority rule\" to determine which group of brokers should remain active. In this scenario, Group B has a majority of brokers (3 out of 5) and can communicate with Zookeeper. Therefore, partitions with a leader in Group B will continue to function normally. For partitions with a leader in Group A, the brokers in Group B will detect the loss of the leader and elect a new leader from the available ISRs within Group - B. This ensures that all partitions have a functioning leader and can continue to serve client requests.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. Not all partitions will be offline in this scenario. As long as a group of brokers (in this case, Group B) has a majority and can communicate with Zookeeper, Kafka will allow that group to continue operating and serving client requests for the partitions they hold the leadership for.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The behavior of partition leadership in a network partition scenario is deterministic and does not depend on which group the Controller belongs to. Kafka's partition leadership election process is designed to prioritize availability and maintain consistency based on the majority of brokers and their ability to communicate with Zookeeper, regardless of the Controller's location.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`num.io.threads` controls the number of threads used for disk I/O operations, `num.network.threads` controls the number of threads used for network I/O operations, and `num.replica.fetchers` controls the number of threads used for fetching messages from the leader replica.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`num.io.threads` controls the number of threads used for network I/O operations, `num.network.threads` controls the number of threads used for disk I/O operations, and `num.replica.fetchers` controls the number of threads used for fetching messages from the leader replica.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`num.io.threads` controls the number of threads used for disk I/O operations, `num.network.threads` controls the number of threads used for network I/O operations, and `num.replica.fetchers` controls the number of threads used for replicating messages to follower replicas.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`num.io.threads` controls the number of threads used for replicating messages to follower replicas, `num.network.threads` controls the number of threads used for disk I/O operations, and `num.replica.fetchers` controls the number of threads used for network I/O operations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. The given configurations control different aspects of the broker's performance and resource utilization:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The description of `num.io.threads` and `num.network.threads` is swapped in this option. `num.io.threads` is used for disk I/O operations, while `num.network.threads` is used for network I/O operations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The description of `num.replica.fetchers` is incorrect in this option. `num.replica.fetchers` controls the number of threads used for fetching messages from the leader replica, not for replicating messages to follower replicas.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The descriptions of all three configurations are incorrect in this option. `num.io.threads` is used for disk I/O operations, `num.network.threads` is used for network I/O operations, and `num.replica.fetchers` is used for fetching messages from the leader replica.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka will start a new log segment when the current segment reaches 1 GB in size or after 24 hours, whichever comes first. Old log segments will be retained for 7 days.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka will start a new log segment when the current segment reaches 1 GB in size or after 24 hours, whichever comes first. Old log segments will be retained indefinitely.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka will start a new log segment when the current segment reaches 1 GB in size. Old log segments will be retained for 7 days or until the total log size exceeds 1 GB, whichever comes first.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka will start a new log segment after 24 hours, regardless of the size. Old log segments will be retained for 7 days or until the total log size exceeds 1 GB, whichever comes first.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Correct**. The given configurations control the log segment creation and retention policies for Kafka:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. While the log segment creation policy is correctly described, the retention policy is incorrect. Old log segments will not be retained indefinitely, as `log.retention.ms` is set to 7 days (604800000 milliseconds).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The log segment creation policy is partially correct, but it ignores the time-based limit specified by `log.segment.ms`. Additionally, the retention policy is incorrect, as `log.retention.bytes=-1` means that there is no size-based retention limit.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Incorrect**. The log segment creation policy is incorrect, as it ignores the size-based limit specified by `log.segment.bytes`. The retention policy is also incorrect, as `log.retention.bytes=-1` means that there is no size-based retention limit.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "1",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "2",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "3",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "4",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "6",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "9",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "12",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "18",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The partition will remain unavailable until the failed leader recovers.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The partition will elect a new leader from the out-of-sync replicas to maintain availability.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The partition will automatically create a new replica to replace the failed leader.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The partition will be reassigned to another broker in the cluster.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If `unclean.leader.election.enable` is set to `true`, Kafka will elect a new leader from the out-of-sync replicas to maintain availability, potentially resulting in data loss or inconsistency.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If `unclean.leader.election.enable` is set to `false`, Kafka will not elect a leader from the out-of-sync replicas and will instead keep the partition unavailable until the failed leader recovers or a new ISR becomes available.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "1 MB",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "4 MB",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "1048576 bytes",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "4194304 bytes",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The log segment is older than 48 hours.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The log segment size exceeds 536870912 bytes (512 MB).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The total size of all log segments for the topic exceeds 1073741824 bytes (1 GB).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "All of the above.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`log.retention.hours=48`: This setting specifies the maximum time a log segment can be retained before it becomes eligible for deletion. In this case, any log segment older than 48 hours will be eligible for deletion, regardless of its size.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`log.segment.bytes=536870912`: This setting determines the maximum size of a single log segment. When a log segment reaches this size (512 MB in this case), Kafka will close the current segment and start a new one. The old segment will be eligible for deletion based on the retention policies.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`log.retention.bytes=1073741824`: This setting specifies the maximum total size of all log segments for a topic. If the total size of all log segments exceeds this value (1 GB in this case), Kafka will start deleting the oldest segments to free up space, even if they haven't reached the time-based retention limit.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Producer-side compression:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Zero-copy data transfer:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Broker-side storage:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consumer-side decompression:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Broker",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "When the first consumer instance starts, it becomes the group leader and triggers a rebalance. It is assigned a subset of the topic partitions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "CLI",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "When subsequent consumer instances start with the same group, they join the group and trigger a rebalance. The partitions are redistributed among all the consumers in the group.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "CLI",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Each consumer instance will consume messages from its assigned partitions independently. Messages from a single partition are processed by only one consumer instance.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "CLI",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If a consumer instance fails or is terminated, the partitions it was consuming are redistributed among the remaining consumers in the group during a rebalance.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "CLI",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "It uses `kafka-topics.sh` instead of `kafka-configs.sh`",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "CLI",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "It uses the deprecated `--zookeeper` option",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "CLI",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The replicas hosted on the failed broker become inaccessible.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "For partitions where the failed broker was hosting the leader replica:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "For partitions where the failed broker was hosting a follower replica:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "When the failed broker is restarted:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Leader replica:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Follower replicas:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "In-sync replicas (ISRs):",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consistency guarantees:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`__consumer_offsets`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`__transaction_state`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`__confluent.support.metrics`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_schemas`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`connect-configs`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`connect-offsets`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`connect-status`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_confluent-command`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_confluent-monitoring`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_confluent-secrets`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`__consumer_timestamps`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_confluent-metrics`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_confluent-telemetry-metrics`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`ksql-clusterksql_processing_log`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`_confluent-license`:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Cluster-Administration",
    "subtopicsJson": "[]",
    "category": "administrator",
    "certification": "Confluent Kafka Administrator",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer calls `assign` with a new collection of `TopicPartition` objects representing the desired partitions to consume from.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka updates the consumer's assignment to the specified partitions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer will stop consuming from its previous assignment and start consuming from the newly assigned partitions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer can continue processing messages from the new partitions without needing to restart.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "It will rejoin the consumer group.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka will reassign partitions to the consumers in the group, including the restarted consumer.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "For each assigned partition, the consumer will resume processing from the last committed offset.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka will assign a subset of the partitions to the new consumer based on the consumer group's partition assignment strategy.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "For each assigned partition, the new consumer will start consuming from the last committed offset.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer sends a commit request to the Kafka broker, specifying the offsets it wants to commit for each partition it is consuming from.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The Kafka broker receives the commit request and updates the committed offsets for the consumer group in its metadata.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The broker sends a response back to the consumer indicating whether the commit was successful or not.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If the commit is successful, the consumer considers the processed messages as committed and will not receive them again even if it restarts.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If the commit fails, the consumer may retry the commit or handle the failure based on its error handling strategy.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "\"read_uncommitted\" (default): With this isolation level, the consumer will read all messages in a partition, including transactional messages that are not yet committed. This means that the consumer may see messages that are part of an ongoing transaction or messages that were part of a transaction that was later aborted.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "\"read_committed\": With this isolation level, the consumer will only read messages that are not part of an ongoing transaction and messages that are part of a committed transaction. It will wait until a transaction is committed before making its messages visible to the consumer. This ensures that the consumer only sees messages that are part of successful transactions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer maintains internal state, such as offset positions and partition assignments, which can become inconsistent if accessed concurrently.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer may rebalance partitions or update its internal state based on the messages processed, and concurrent access can interfere with these operations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The behavior of concurrent access to the consumer is not defined and may vary depending on the Kafka version, the JVM implementation, or other factors.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Create a separate thread for each consumer instance.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "In each thread, create a new KafkaConsumer instance and configure it with the desired properties (e.g., bootstrap servers, group ID, deserializers).",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Subscribe each consumer instance to the topic(s) you want to consume from.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "In each thread, continuously call `poll()` on the consumer instance to retrieve messages and process them independently.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement proper error handling and resource cleanup in each thread to handle exceptions and gracefully shutdown the consumers when necessary.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "When a consumer group is created or a new consumer instance joins the group, Kafka initiates a rebalance operation.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "During the rebalance, Kafka assigns the partitions of the subscribed topics to the consumer instances in the group.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The assignment is done using a round-robin approach, where each consumer instance is assigned a subset of the partitions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If there are more partitions than consumer instances, some consumer instances may be assigned multiple partitions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If there are more consumer instances than partitions, some consumer instances may not be assigned any partitions and will remain idle.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "As messages are produced to the partitions, each consumer instance processes the messages from its assigned partitions independently.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If a consumer instance fails or leaves the group, Kafka triggers a rebalance to redistribute the partitions among the remaining consumer instances.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Reduced memory overhead:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Improved performance:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Efficient resource utilization:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`read_uncommitted` (default): With this isolation level, the consumer will read all messages, including transactional messages that are not yet committed. It may read messages from aborted transactions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`read_committed`: With this isolation level, the consumer will only read messages that are not part of ongoing transactions and messages that are part of committed transactions. It will wait for transactions to be committed before making the messages visible to the consumer.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer connects to any available broker in the Kafka cluster.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer sends a metadata request to the connected broker, specifying the topic and partition it wants to read from.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The broker responds with the metadata information, including the current leader broker for the specific partition.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer disconnects from the initial broker and establishes a new connection to the leader broker for the partition.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The consumer starts reading data from the leader broker for the specific partition.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Definition**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**In-Sync Replicas (ISR)**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Replication and Acknowledgements**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Message Visibility**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Consumer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consumer + Producer",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Source",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Sink",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Streams",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consumer + Producer",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Source",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Sink",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Streams",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consumer + Producer",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Source",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Sink",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Streams",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Utilize a mainframe connector with Kafka Connect to ingest the transaction data into a Kafka topic. Apply a Kafka Streams application to anonymize sensitive information in the stream before conducting fraud analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly connect the mainframe system to the Kafka Streams application using a custom API, ensuring sensitive data is filtered out during the streaming process.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Employ ksqlDB to directly query the mainframe system, apply data anonymization functions to filter out sensitive information, and then write the sanitized data to a Kafka topic for stream processing.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement a batch process to extract transaction data periodically from the mainframe, cleanse the data of sensitive information, and then load the sanitized data into Kafka topics for streaming analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Deploy a Kafka Connect Source Connector to capture session data directly into Kafka, using Stream Processing to anonymize user identifiers before aggregating sessions for recommendation analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use a custom Kafka Producer application to publish session data to a topic, applying a Stream Processor to anonymize and then process the data for generating recommendations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement a Kafka Connect Sink Connector to store session data into a NoSQL database, with a pre-processor to remove sensitive information before ingestion. Use Kafka Streams to read from the database for analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Create a ksqlDB process to pull session data from source systems, apply anonymization functions within ksqlDB, and output the clean data to a Kafka topic for further processing by the Kafka Streams application.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use Kafka Connect with appropriate Source Connectors for each metric source, configuring the connectors to filter out proprietary information. Process the filtered metrics stream with Kafka Streams for alerting.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly stream all metrics into Kafka using custom Producers, then employ a Kafka Streams application to separate proprietary data from non-proprietary data, and analyze the latter for alerting.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement a series of ksqlDB statements to ingest metrics into Kafka, applying filtering logic within ksqlDB to remove proprietary information before streaming processing and alerting.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure a Kafka Connect Sink Connector to aggregate all metrics into a centralized database, followed by batch processing to remove proprietary information before streaming the data into Kafka for real-time analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Deploy Kafka Streams applications near each regional system to collect and forward data to the centralized Kafka topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use Kafka Connect with a mix of Source Connectors suitable for each regional system's technology to ingest data directly into Kafka.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement custom Kafka Producers embedded within each regional system to push data to the centralized Kafka topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure a Kafka Connect Sink Connector for each regional system to replicate data into the centralized Kafka topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Utilize a Kafka Connect Source Connector to ingest interaction data into Kafka, then process this data with Kafka Streams to update content recommendations in real-time.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly stream interaction data into Kafka using a custom API, then use ksqlDB to perform real-time analysis and generate content recommendations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement batch processing jobs to periodically analyze interaction data stored in an external database, and then use Kafka to distribute batch analysis results for content recommendation updates.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure Kafka Connect Sink Connectors to collect interaction data into a big data platform first, then process the data using external stream processing tools before updating content recommendations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure Kafka Connect Sink Connectors to collect telemetry data from the IoT devices into Kafka, followed by a Kafka Streams application to aggregate and analyze the data.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use Kafka Connect Source Connectors appropriate for the IoT devices' communication protocols to ingest telemetry data into Kafka, then employ Kafka Streams for data aggregation and anomaly detection.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Develop custom Kafka Producers within the IoT devices to send data directly to Kafka topics, then use external tools to pull and analyze the data from Kafka.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement a centralized database to collect IoT telemetry data first, then use Kafka Connect Source Connectors to ingest the data from the database into Kafka for further processing.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Dynamically adjust the number of partitions in the user activities topic based on incoming data volume.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Scale the Kafka Streams application instances up or down in response to the processing load.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increase or decrease the number of Kafka Connect Source Connector tasks to match the rate of incoming user activity data.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Modify the replication factor of the user activities topic during high load periods to improve data durability and availability.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Automatically adjust the number of topics to spread the increased order messages across more Kafka topics during sales events.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use Kafka Connect with scalable Source Connectors to adjust the throughput based on order volume.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Scale out the Kafka broker cluster by adding more brokers during high-volume periods and scale in when the volume decreases.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure the Kafka producer to dynamically adjust batch size and linger time based on the current throughput of order messages.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Adjust the replication factor of the watch history topic in real-time to handle the increased data volume.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increase and decrease the number of Kafka Connect Sink Connector tasks to efficiently write watch history data into Kafka.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Scale the number of Kafka Streams applications processing the watch history data according to the ingestion rate.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Dynamically modify the number of partitions in the watch history topic to manage the load during peak engagement times.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Adjusting the replication factor in real-time is not practical and does not directly address the issue of variable ingestion rates. Replication factor affects data redundancy and fault tolerance, not processing capacity.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect Sink Connectors are used to export data from Kafka to external systems, not for ingesting data into Kafka. This option is not relevant for the given scenario of ingesting viewer watch history.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Dynamically modifying the number of partitions is operationally complex and can cause data rebalancing issues. It's not recommended to change partition counts frequently in response to fluctuating loads.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use JDBC Source Connectors to ingest container tracking data into Kafka. Transform this data into a unified format using Kafka Streams for real-time logistics management.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Streamline data into Kafka using custom scripts that extract data from databases and publish to Kafka, relying on Kafka Streams for necessary transformations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consolidate data in the RDBMS using SQL procedures to match the target schema required by Kafka, then use JDBC Source Connectors to stream this unified data into Kafka.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ingest raw data into Kafka using JDBC Source Connectors, then employ ksqlDB to perform SQL-like transformations and route the data to various microservices.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use JDBC Source Connectors to ingest transaction data into Kafka, followed by Kafka Streams for data transformation and enrichment.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly export transaction data to CSV files, use custom producers to send these files to Kafka, and then apply Kafka Streams for transformation.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ingest transaction data into Kafka using JDBC Source Connectors and leverage ksqlDB for transforming and querying the data in a SQL-like manner.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Transform data within the legacy system using SQL stored procedures, then use JDBC Source Connectors to ingest the pre-transformed data into Kafka.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Utilize JDBC Source Connectors to stream sensor data into Kafka, transforming the data into a more accessible format using Kafka Streams.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Export sensor data to a common format like JSON, use Kafka Connect to ingest this data, and then apply ksqlDB to analyze and visualize the data in real-time.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Stream sensor data directly into Kafka using custom producers, followed by data transformation through SQL procedures embedded within the Kafka ecosystem.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ingest sensor data into Kafka using JDBC Source Connectors, then use ksqlDB to perform real-time analytics and transform the data for downstream processing.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "JDBC Source Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "S3 Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Elasticsearch Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "HDFS Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "JDBC Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Elasticsearch Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "MongoDB Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Connect File Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "JDBC Source Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "S3 Sink Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "MQTT Source Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "File Source Connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ingest sensor data into a Kafka topic using MQTT connectors. Separately, use an external service to fetch weather forecasts, storing this data in a Kafka topic via the HTTP Source Connector. Utilize Kafka Streams to join sensor data with weather forecasts based on location and timestamp, outputting enriched data to another topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly stream sensor data and weather forecasts into Kafka using custom Kafka Producers implemented in Python. These producers perform API calls to retrieve forecasts, merge this data with sensor readings, and produce the combined records into a single Kafka topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use the MQTT Source Connector to ingest sensor data into Kafka. Write a Kafka Streams application that performs REST API calls to the weather forecast service for each sensor data record processed, enriching and producing enriched records to a new topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement an MQTT proxy to capture sensor data into Kafka. Concurrently, utilize Kafka Connect with the JDBC Sink Connector to store sensor data in a relational database, from which an external cron job fetches weather forecasts, merges them with sensor data, and re-ingests the enriched data back into Kafka via JDBC Source Connector.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Deploy Kafka Connect with the JDBC Source Connector at each store to ingest sales data into Kafka, using Single Message Transforms (SMTs) to filter and reduce the size of the data on the fly. Aggregate this data centrally using a Kafka Streams application for inventory analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Utilize log-based Change Data Capture (CDC) connectors to monitor changes in each store's SQL database, streaming only new or changed sales records into Kafka. This minimizes network usage and enables real-time central processing with Kafka Streams.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement custom Kafka Producers within the POS systems to directly publish sales data to Kafka, compressing messages to mitigate network bandwidth issues. Use Kafka Streams for processing and inventory management centrally.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Set up a central database to aggregate sales data from all stores nightly. Use the JDBC Sink Connector to transfer this aggregated data into Kafka for next-day inventory analysis, relying on batch processing rather than real-time analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use Kafka Connect with a custom NoSQL Source Connector for each geographical location to ingest logs into Kafka, then utilize Kafka Streams for real-time analysis and dynamic content delivery.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly stream logs from NoSQL databases to Kafka using log-based Change Data Capture (CDC) connectors specific to each NoSQL database type, followed by processing with ksqlDB to generate viewer insights and personalized content recommendations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure a network of MQTT brokers to collect logs from each location, and then use an MQTT Source Connector to consolidate logs into Kafka. Apply a Kafka Streams application to analyze viewer interactions and adjust content recommendations.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement a batch ETL process to extract logs from NoSQL databases nightly, load them into Kafka for next-day processing with Kafka Streams, and update content recommendations based on the analysis.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configure MongoDB Source Connector to capture new and updated reviews into Kafka, then use Kafka Streams for sentiment analysis and to adjust product rankings in near-real-time.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Utilize a cron job to export reviews from MongoDB to CSV files at regular intervals, then use File Source Connectors to ingest these files into Kafka, followed by processing with Kafka Streams.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Deploy log-based CDC connectors to stream only the changes (new and updated reviews) from MongoDB into Kafka, leveraging ksqlDB for continuous sentiment analysis and product ranking adjustments.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly access the MongoDB API from Kafka Streams applications to fetch new and updated reviews, perform sentiment analysis, and update product rankings without storing reviews in Kafka.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use JDBC Source Connectors to ingest transaction data from legacy databases into Kafka. Enrich the data using Kafka Streams by joining it with real-time fraud detection signals. Utilize a Kafka Connect Sink Connector to publish the enriched data to the dashboard.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Implement custom Kafka Producers to extract transaction data from legacy systems, enriching the data in-flight with fraud detection signals before producing it to a Kafka topic for dashboard consumption.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Directly connect the legacy databases to Kafka Streams applications using custom database clients. Perform the enrichment with real-time fraud detection signals in the application, then produce the enriched data to a Kafka topic for dashboard visualization.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Leverage Change Data Capture (CDC) connectors to stream transaction data from legacy databases into Kafka. Use Kafka Streams for real-time enrichment with fraud detection signals and ksqlDB to further process and prepare the data for dashboard presentation.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "A Source Connector is needed to import data from the source system into Kafka topics.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Kafka Streams can then process this data in real-time.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Finally, a Sink Connector is needed to export the processed results from Kafka topics to HDFS.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The connector will create one task per table, ignoring the `max.tasks` setting",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The connector will create tasks up to the `max.tasks` limit, potentially leaving some tables without dedicated tasks",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The connector will distribute the tables evenly among the available tasks",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The connector will fail with an error due to the insufficient number of tasks",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increase the `max.tasks` configuration of the connector",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increase the number of partitions in the target Kafka topic",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increase the `tasks.max` configuration of the Kafka Connect workers",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use multiple instances of the JDBC connector, each copying a different subset of tables",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increase the `max.tasks` configuration of the connector:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Use multiple instances of the JDBC connector, each copying a different subset of tables:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Offset Tracking:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Fault Tolerance:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Connector Recovery:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Connector Configuration:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Task Allocation:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Scalability:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Resource Utilization:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Worker Node Failure Detection:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Task Rebalancing:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Connector and Task Reassignment:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Continuous Operation:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Connect",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ensuring both topics have the same `replication.factor` to prevent data loss.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Using a custom partitioner that assigns messages to the same partition number in both topics based on key.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configuring `TopicB` with a higher number of partitions than `TopicA` to ensure scalability.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Increasing `max.poll.records` to a higher value to ensure more messages are processed in each poll.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Configuring both topics with `log.compaction=true` to ensure message deduplication.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ensuring that both topics are consumed by the application in separate threads.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Assigning a unique consumer group for each topic to maximize parallel processing.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ensuring both topics use the same key for related messages and are consumed by the same consumer group.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "compression.type = snappy",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "cleanup.policy = delete",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "cleanup.policy = compact",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Key Serde = Serdes.String(), Value Serde = Serdes.BigDecimal()",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Key Serde = Serdes.Long(), Value Serde = Serdes.Double()",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Key Serde = Serdes.String(), Value Serde = Serdes.String()",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "retention.bytes = -1",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "cleanup.policy = compact",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "min.insync.replicas = 2",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "state.store.replication.factor = 3",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "state.store.log.compaction = true",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "commit.interval.ms = 100",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Aggregate: Aggregation operations, such as counting, summing, or averaging values, are stateful because they need to maintain a running state of the aggregated result. The state is updated as new messages arrive, and the final result depends on the accumulated state over time.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Join: Join operations, especially window-based joins, are stateful because they need to maintain a state of the messages from both streams within a specified time window. The stream processor must store the messages temporarily to match and combine them based on a common key.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Persistence: State stores allow the streaming application to persist the intermediate state to disk. This ensures that the state is not lost if the application fails or needs to be restarted. When the application restarts, it can reload the state from the state stores and resume processing from where it left off.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Fault Tolerance: By persisting the state, state stores enable fault tolerance in Kafka Streams applications. If a node in the Kafka Streams cluster fails, another node can take over the processing and recover the state from the state stores. This ensures that the processing can continue without losing the accumulated state.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Queryable State: State stores in Kafka Streams also provide the ability to query the current state of the application. This allows other applications or services to retrieve the latest computed state without needing to process the entire stream again. Queryable state is useful for serving real-time results or building interactive applications.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Change Log Topic: For each state store in a Kafka Streams application, Kafka Streams creates a corresponding change log topic. The change log topic acts as a persistent log of all the state changes that occurred in the state store.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "State Updates: Whenever the state in a state store is updated as a result of processing messages, Kafka Streams writes the state changes to the change log topic. Each record in the change log topic represents a state update and includes the key, value, and timestamp of the update.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Failure Recovery: If a failure occurs and a Kafka Streams application needs to recover its state, it starts by reading the change log topic from the beginning. The application replays the state changes from the change log topic to rebuild the state store. By replaying the state changes in the correct order, the application can restore its state to the latest consistent point before the failure.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Resuming Processing: Once the state is recovered from the change log topic, the Kafka Streams application can resume processing from the point where it left off. It continues to read input messages from the source topics and applies the processing logic to update the state and generate output.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Same Number of Partitions**: Co-partitioned topics must have the same number of partitions. This ensures that data which is logically related and keyed similarly ends up in the corresponding partition across these topics.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Consistent Partitioning Strategy**: To ensure that related messages from different topics land in the corresponding partitions, a consistent partitioning strategy must be used. This often involves ensuring that messages are produced with the same key and that the default partitioner or a custom partitioner is used consistently across these topics.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Parallel Consumer Processing**: When consuming from co-partitioned topics, ensure that the consumer groups are configured to consume in parallel from all the related partitions. This is typically handled naturally by Kafka's consumer group mechanism when the topics are consumed together.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Kafka Streams and KTable**: In Kafka Streams, co-partitioning is essential when performing join operations between KStreams and KTables or between KStreams themselves. Kafka Streams applications will automatically co-partition the data of joined streams by repartitioning them as needed, which may introduce additional processing steps and topics.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Use Case for Co-Partitioning**: Co-partitioning is often used in scenarios requiring data aggregation or join operations across different data streams that share a logical relationship, such as aggregating user click events with user profile information for real-time analytics.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Management and Maintenance**: When scaling out (adding more partitions to topics), ensure that all co-partitioned topics are scaled consistently. This might require manual intervention or custom tooling, as Kafka does not automatically manage the partition count across topics to maintain co-partitioning.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Producing to Co-Partitioned Topics**: When producing messages to co-partitioned topics, ensure that the keys used for partitioning are consistent and that messages destined to be joined together use the same keys.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Writes the repartitioned data to a new topic with new keys and partitions.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Creates a new set of tasks to read and process events from the new topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Kafka-Streams",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Headless ksqlDB deployment (Application Mode):**",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Interactive ksqlDB deployment:**",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "A team of users develops and tests their queries interactively on a shared testing `ksqlDB` cluster using the CLI.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "When ready for production deployment, the verified queries are version-controlled and stored in a `.sql` file.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The `.sql` file is provided to the `ksqlDB` servers during startup using either:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Source queries**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Persistent queries**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Push queries**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "**Pull queries**:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "KSQL",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If the number of in-sync replicas for a partition drops below `min.insync.replicas`, the broker will start rejecting writes to that partition. If this happens, and the producer exhausts its retries, the write will fail and the data will be lost. This can happen if replicas crash or become unavailable.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If `unclean.leader.election.enable=true` and all in-sync replicas for a partition fail, an out-of-sync replica can be elected as the new leader. This replica may be missing some of the latest messages, causing data loss.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If the producer crashes (or loses connectivity) after the broker acknowledges a write but before the producer records the acknowledgment, the producer will treat the write as failed and may retry it. This can lead to duplicate messages, but from the perspective of the crashed producer instance, the original message is lost.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Producer-side serialization:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Zero-copy data transfer:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Consumer-side deserialization:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The producer calculates the hash of the message key using the murmur2 hash function.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The hash value is then modulo'd by the number of partitions in the topic to determine the partition index.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The message is sent to the corresponding partition based on the calculated partition index.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The producer maintains an internal counter that keeps track of the last partition it sent a message to.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "When a message without a key is sent, the producer increments the counter and selects the next partition in a round-robin fashion.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The message is sent to the selected partition.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The counter is incremented again, and the process repeats for subsequent messages.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Producer",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "It retrieves the schema corresponding to the specified ID from the Schema Registry.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "REST Proxy",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "It validates the message payload against the retrieved schema to ensure that the payload adheres to the schema structure.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "REST Proxy",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If the validation succeeds, the REST Proxy produces the message to the specified Kafka topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "REST Proxy",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "If the validation fails, the REST Proxy returns an error indicating that the payload does not match the schema.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "REST Proxy",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Apache Avro: Avro is a row-based serialization format that is compact, fast, and binary. It's the most commonly used format with the Schema Registry.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Protocol Buffers (Protobuf): Protobuf is Google's data interchange format. It's also compact and fast, and it supports schema evolution.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Java: The Java client is part of the `kafka-schema-registry-client` library.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Python: The Python client is provided by the `confluent-kafka` Python package.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Go: The Go client is part of the `confluent-kafka-go` package.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Surround the code that consumes and processes the messages with a try-catch block.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "In the catch block, if the exception is a SerializationException, log an error message indicating the failed deserialization.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Commit the offset of the problematic message using the consumer's commitSync() or commitAsync() method. This tells Kafka that the consumer has processed the message, even though it couldn't deserialize it.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Continue consuming the next message.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Automatic schema evolution and compatibility checks:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Improved deserialization performance compared to generic deserializers:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Ability to deserialize messages without knowing the schema upfront:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Removing a field",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Making a required field optional",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Widening the range of an integer or long field",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Adding a new optional field",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Adding a new required field with a default value",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Schema-Registry",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The updated configuration is stored in Zookeeper.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Topic",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The Kafka brokers read the updated configuration from Zookeeper and apply the changes to the topic.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Topic",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "The changes take effect immediately without requiring a restart of the Kafka brokers.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Topic",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Delete: This is the default policy. When the retention time or size limit is reached, Kafka deletes old log segments to free up space. This means that old messages are permanently removed based on the retention configuration.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Topic",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "Compact: With the compact policy, Kafka periodically compacts the log by removing obsolete records based on the message key. If a key appears multiple times in the log, only the latest value is retained, and the older duplicates are discarded. This is useful for maintaining a changelog or snapshot of the latest state for each key.",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Topic",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`random-uuid`: Generates a new cluster ID for the Kafka cluster. For example:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "",
    "topic": "Zookeeper",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  },
  {
    "content": "`format`: Formats the storage directories for each broker and controller using the cluster ID. For example:",
    "optionsJson": "[]",
    "correctAnswer": -1,
    "explanation": "****",
    "topic": "Zookeeper",
    "subtopicsJson": "[]",
    "category": "developer",
    "certification": "Confluent Kafka Developer",
    "importance": 1,
    "answer": ""
  }
]